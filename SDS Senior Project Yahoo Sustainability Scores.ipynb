{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea65fc3",
   "metadata": {},
   "source": [
    "# Code for scraping Yahoo Finance\n",
    "\n",
    "This file contains two main chunks of code, one for getting sustainability data from Yahoo Finance, and the other for getting industry and employee count from Yahoo Finance. All relevant files are in the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0dfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "# !pip install html5lib\n",
    "import urllib.request\n",
    "# !pip install pywebcopy \n",
    "# !pip install pyquery\n",
    "# !pip install w3lib\n",
    "# !pip install parse \n",
    "# !pip install lxml\n",
    "from pywebcopy import save_webpage\n",
    "import lxml\n",
    "from lxml import html\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab752856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4588"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a list of all the tickers from the SEC 10-K dataset\n",
    "ticker_list = pd.read_csv(\"/Users/phoebeliu/Downloads/SEC_Kaggle_Tickers.csv\", index_col = False)\n",
    "ticker_list = ticker_list[\"0\"]\n",
    "ticker_list[:10]\n",
    "len(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca7c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of all URLs \n",
    "# sample URL from which we're trying to get sustainalytics data\n",
    "    # https://finance.yahoo.com/quote/AAPL/sustainability?p=AAPL\n",
    "    # https://finance.yahoo.com/quote/FKYS/sustainability?p=FKYS\n",
    "url_list = []\n",
    "for ticker in ticker_list:\n",
    "    url1 = \"https://finance.yahoo.com/quote/\"\n",
    "    url2 = \"/sustainability?p=\"\n",
    "    url_list.append(f\"https://finance.yahoo.com/quote/{ticker}/sustainability?p={ticker}\")\n",
    "\n",
    "# convert to DF because we want to save the file\n",
    "url_list = pd.DataFrame(url_list, columns = [\"URL\"])\n",
    "\n",
    "# add tickers to it \n",
    "ticker_df = pd.DataFrame(np.array(ticker_list), columns = [\"Ticker\"])\n",
    "url_list = pd.concat([ticker_df, url_list], axis=1)\n",
    "\n",
    "url_list.to_csv(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooURLList.csv\")\n",
    "ticker_df.to_csv(\"/Users/phoebeliu/Documents/SDS_Senior_Project/TickerList.csv\")\n",
    "url_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4963de3",
   "metadata": {},
   "source": [
    "# Get/save the HTML files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from SDS_Senior_Project import fleepy\n",
    "import time\n",
    "# url_list.to_html(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooURLList.csv\")\n",
    "\n",
    "def gen_rows(df):\n",
    "    for row in df.itertuples(index=False):\n",
    "        yield row._asdict()\n",
    "        \n",
    "for row in gen_rows(url_list):\n",
    "    fleepy.foo(row)\n",
    "    time.sleep(2.5)\n",
    "    \n",
    "# with Pool() as p:\n",
    "#         p.map(fleepy.foo, gen_rows(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure it worked\n",
    "len(listdir(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooFinance_HTMLs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec4ab0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# opening csv file and setting up fields with BeautifulSoup\n",
    "csv_file = open(\"/Users/phoebeliu/Documents/SDS_Senior_Project/yahoo_sustainalytics_score.csv\", \"w\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['ticker','total ESG score', 'percentile', \"e_score\", \"s_score\", \"g_score\", \"controversy\"])\n",
    "all_htmls = listdir(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooFinance_HTMLs\")\n",
    "\n",
    "count = 0\n",
    "# looping through all the tickers in tickerlist\n",
    "for ticker_file in all_htmls:\n",
    "    # all_htmls[1].split('.')[0]\n",
    "    temp_ticker = ticker_file.split('.')[0]\n",
    "    with open(r'/Users/phoebeliu/Documents/SDS_Senior_Project/YahooFinance_HTMLs/' + ticker_file, \"r\") as f:\n",
    "        page = f.read()\n",
    "    soup = BeautifulSoup(page, \"html\")\n",
    "    datapanel = soup.find(\"div\",{\"class\": \"Pb(20px) Bdbs(s) Bdbw(3px) Bdbc($seperatorColor) smartphone_Pb(10px) smartphone_Px(20px) smartphone_Bdbw(4px) smartphone_Bdbc($seperatorColor)\"})\n",
    "    if datapanel is not None:\n",
    "        total_score = datapanel.find(\"div\", {\"class\": \"Fz(36px) Fw(600) D(ib) Mend(5px)\"}).text.lstrip()\n",
    "        percentile = datapanel.find(\"span\",{\"class\": \"Bdstarts(s) Bdstartw(0.5px) Pstart(10px) Bdc($seperatorColor) Fz(12px) smartphone_Bd(n) Fw(500)\"}).text.lstrip()\n",
    "\n",
    "        controversy = soup.find(\"div\",{\"class\": \"D(ib) Fz(36px) Fw(500)\"})\n",
    "        if controversy is not None:\n",
    "            controversy = controversy.text.lstrip()\n",
    "        else:\n",
    "            controversy = \"\"\n",
    "        components = datapanel.find_all(\"div\",{\"class\": \"D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)\"})\n",
    "        if len(components) > 0:\n",
    "            env_score = components[0].text.lstrip()\n",
    "            social_score = components[1].text.lstrip()\n",
    "            governance_score = components[2].text.lstrip()\n",
    "        else:\n",
    "            env_score = \"\"\n",
    "            social_score = \"\"\n",
    "            governance_score = \"\"\n",
    "        \n",
    "        csv_writer.writerow([temp_ticker, total_score, percentile, env_score, social_score, governance_score, controversy])\n",
    "        # print(total_score, percentile, env_score, controversy)\n",
    "    else:\n",
    "        print(temp_ticker)\n",
    "        print(count)\n",
    "        count = count+1\n",
    "    # else don't write the row in the csv\n",
    "          \n",
    "csv_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bdafc6",
   "metadata": {},
   "source": [
    "# Do the same thing but for the other data (industry/headcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd68104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all URLs \n",
    "# sample URL from which we're trying to get sustainalytics data\n",
    "    # https://finance.yahoo.com/quote/AAPL/profile?p=AAPL\n",
    "url_list_profiles = []\n",
    "for ticker in ticker_list:\n",
    "    url_list_profiles.append(f\"https://finance.yahoo.com/quote/{ticker}/profile?p={ticker}\")\n",
    "\n",
    "# convert to DF because we want to save the file\n",
    "url_list_profiles = pd.DataFrame(url_list_profiles, columns = [\"URL\"])\n",
    "\n",
    "# add tickers to it \n",
    "ticker_df = pd.DataFrame(np.array(ticker_list), columns = [\"Ticker\"]) # don't think i need this bc it was from\n",
    "# the previous thing\n",
    "url_list_profiles = pd.concat([ticker_df, url_list_profiles], axis=1)\n",
    "url_list_profiles.to_csv(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooURLList_Profiles.csv\")\n",
    "url_list_profiles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from SDS_Senior_Project import turt\n",
    "import time\n",
    "# url_list.to_html(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooURLList.csv\")\n",
    "\n",
    "def gen_rows(df):\n",
    "    for row in df.itertuples(index=False):\n",
    "        yield row._asdict()\n",
    "        \n",
    "for row in gen_rows(url_list_profiles):\n",
    "    turt.foo(row)\n",
    "    time.sleep(2.5)\n",
    "    \n",
    "# with Pool() as p:\n",
    "#         p.map(fleepy.foo, gen_rows(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllist_p2 = pd.read_csv(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooURLList_Profiles_Sub.csv\")\n",
    "urllist_p2[\"URL\"][:10]\n",
    "\n",
    "for row in gen_rows(urllist_p2):\n",
    "    turt.foo(row)\n",
    "    time.sleep(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5dc2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDN\n",
      "TVTY\n",
      "AGGX\n",
      "XSPA\n",
      "AVCO\n",
      "CDEV\n",
      "TYME\n",
      "REGI\n",
      "TLGTQ\n",
      "MNKKQ\n",
      "GCC\n",
      "UNSS\n",
      "FRLI\n",
      "SAIL\n",
      "POLY\n",
      "UNL\n",
      "PEYE\n",
      "BDR\n",
      "WSFT\n",
      "ARNA\n",
      "APTS\n",
      "PING\n",
      "SPIN\n",
      "GNBT\n",
      "BDRY\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m temp_ticker \u001b[38;5;241m=\u001b[39m ticker_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/phoebeliu/Documents/SDS_Senior_Project/YahooFinance_Profile_HTMLs/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ticker_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 15\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# panel 1:\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte"
     ]
    }
   ],
   "source": [
    "# opening csv file and setting up fields\n",
    "csv_file_p = open(\"/Users/phoebeliu/Documents/SDS_Senior_Project/yahoo_company_profile_2.csv\", \"w\")\n",
    "csv_writer_p = csv.writer(csv_file_p)\n",
    "csv_writer_p.writerow(['ticker', 'sector', \"industry\", \"employees\", \"description\"])\n",
    "all_htmls_p = listdir(\"/Users/phoebeliu/Documents/SDS_Senior_Project/YahooFinance_Profile_HTMLs\")\n",
    "\n",
    "# need to go back through and clean things\n",
    "\n",
    "count = 0\n",
    "# looping through all the tickers in tickerlist\n",
    "for ticker_file in all_htmls_p:\n",
    "    # all_htmls[1].split('.')[0]\n",
    "    temp_ticker = ticker_file.split('.')[0]\n",
    "    with open(r'/Users/phoebeliu/Documents/SDS_Senior_Project/YahooFinance_Profile_HTMLs/' + ticker_file, \"r\") as f:\n",
    "        page = f.read()\n",
    "    soup = BeautifulSoup(page, \"html\")\n",
    "    \n",
    "    # panel 1:\n",
    "    if soup.find(\"div\",{\"class\": \"Mb(25px)\"}) is None:\n",
    "        # print(temp_ticker)\n",
    "        continue\n",
    "    datapanel = soup.find(\"div\",{\"class\": \"Mb(25px)\"})\n",
    "\n",
    "    # print(datapanel)\n",
    "    # print(datapanel.find(\"p\", attrs={'class' : 'D(ib) W(47.727%) Pend(40px)'})) \n",
    "    # address_info = datapanel.find(\"p\", attrs={'class' : 'D(ib) W(47.727%) Pend(40px)'}).get_text(separator='|')\n",
    "    if datapanel.find(\"p\", attrs={'class' : 'D(ib) Va(t)'}) is not None:\n",
    "        industry_info = datapanel.find(\"p\", attrs={'class' : 'D(ib) Va(t)'}).get_text(separator='|')\n",
    "        industry_info = industry_info.split('|')\n",
    "    else:\n",
    "        print(temp_ticker)\n",
    "        continue\n",
    "\n",
    "    if (len(industry_info) > 2):\n",
    "        sector = industry_info[2]\n",
    "    else:\n",
    "        sector = \"\"\n",
    "    if (len(industry_info) >5):\n",
    "        industry = industry_info[5]\n",
    "    else:\n",
    "        industry = \"\"\n",
    "    if (len(industry_info) > 8):\n",
    "        n_employees = industry_info[8]\n",
    "    else:\n",
    "        n_employees = \"\"\n",
    "        \n",
    "    if soup.find(\"p\", attrs={'class':\"Mt(15px) Lh(1.6)\"}) is not None:\n",
    "        description = soup.find(\"p\", attrs={'class':\"Mt(15px) Lh(1.6)\"}).text\n",
    "    else:\n",
    "        description = \"\"\n",
    "\n",
    "\n",
    "    csv_writer_p.writerow([temp_ticker, sector, industry, n_employees, description])\n",
    "          \n",
    "csv_file_p.close()\n",
    "\n",
    "# this worked for like 3000, try again for more\n",
    "# the ones that don't work\n",
    "# UDN etc. -- it's a fund (should i look at funds ???? )\n",
    "# TVTY -- no longer a public company\n",
    "# AGGX -- just blank\n",
    "# XSPA -- is not a public company\n",
    "# CDEV, MNKKQ -- changed its ticker (without the KQ on yahoo finance)\n",
    "# TLGTQ -- also seems fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0cdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python397jvsc74a57bd03502350be293ee0e4e16dc2779ddec7de1b1d1f391ab72e59acdff9d28e20972"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
